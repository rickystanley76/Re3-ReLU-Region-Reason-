{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed6972-3ace-4781-a936-2117224eed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Import Required Libraries\n",
    "# ==============================\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# PyTorch core modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, ndcg_score\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# Explainability libraries\n",
    "from lime import lime_tabular\n",
    "import shap\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f0cd4-a2dc-4a13-a983-70e9d237f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Load Dataset\n",
    "# ==============================\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "# Note: Ensure that file is located in the working directory\n",
    "df = pd.read_csv('iris.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793490b0-5779-4099-9d80-6f5331fe8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Encode Labels and Split Features/Targets\n",
    "# ==============================\n",
    "\n",
    "# Initialise a LabelEncoder to convert string labels (e.g., 'setosa', 'versicolor', 'virginica')\n",
    "# into numerical labels (0, 1, 2).\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Transform the last column (species names) into numeric values and store in a new column 'label'\n",
    "df['label'] = le.fit_transform(df.iloc[:, -1])\n",
    "\n",
    "# Drop the original string label column (second-to-last column) after encoding\n",
    "df = df.drop(columns=[df.columns[-2]])\n",
    "\n",
    "# Define features (X) and labels (y)\n",
    "X = df.iloc[:, :-1].values   # Select all columns except 'label' → feature matrix\n",
    "y = df['label'].values       # Select the 'label' column → target vector (0,1,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618cadb-22ec-4f47-8fc1-918f6d5cb0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Train/Test Split and Data Preparation\n",
    "# ==============================\n",
    "\n",
    "# Split the dataset into training and test sets (80% / 20%).\n",
    "# 'stratify=y' ensures class distribution is preserved in both sets.\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Initialise a StandardScaler and fit it on the training features.\n",
    "# This standardises features (zero mean, unit variance) to improve training stability.\n",
    "scaler = StandardScaler().fit(X_train_raw)\n",
    "\n",
    "# Apply the fitted scaler to both training and test features\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test  = scaler.transform(X_test_raw)\n",
    "\n",
    "# Convert numpy arrays into PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)   # Features → float32\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)      # Labels → integers (long)\n",
    "X_test  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "# Wrap tensors into TensorDataset objects and create DataLoaders\n",
    "# - train_loader: batches of training data, shuffled\n",
    "# - test_loader: batches of test data, not shuffled\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "test_loader  = DataLoader(TensorDataset(X_test,  y_test),  batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccdb5f-6d12-486a-a724-1dbacb536255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Define Neural Network Model\n",
    "# ==============================\n",
    "\n",
    "# Create a feedforward neural network using nn.Sequential\n",
    "# - Input layer: 4 features (Iris measurements)\n",
    "# - Hidden layers: three fully connected (Linear) layers with ReLU activations\n",
    "# - Output layer: 3 neurons (one for each Iris class)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 16),  # Input → Hidden layer 1 (4 inputs → 16 hidden units)\n",
    "    nn.ReLU(),         # Activation function\n",
    "    nn.Linear(16, 8),  # Hidden layer 1 → Hidden layer 2 (16 → 8 units)\n",
    "    nn.ReLU(),         # Activation function\n",
    "    nn.Linear(8, 4),   # Hidden layer 2 → Hidden layer 3 (8 → 4 units)\n",
    "    nn.ReLU(),         # Activation function\n",
    "    nn.Linear(4, 3)    # Hidden layer 3 → Output layer (4 → 3 classes)\n",
    ")\n",
    "\n",
    "# Display the architecture of the model\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74925b84-0a2d-4781-a162-a38219dec76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Training Loop with Weight History Tracking (3 Hidden Layers)\n",
    "# ==============================\n",
    "\n",
    "# Lists to store weight/bias snapshots for each layer\n",
    "w1_history, b1_history = [], []   # Layer 1 (4 → 16)\n",
    "w2_history, b2_history = [], []   # Layer 2 (16 → 8)\n",
    "w3_history, b3_history = [], []   # Layer 3 (8 → 4)\n",
    "w4_history, b4_history = [], []   # Output Layer (4 → 3)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record weights/biases after each update\n",
    "        w1_history.append(model[0].weight.data.clone().cpu().numpy())  \n",
    "        b1_history.append(model[0].bias.data.clone().cpu().numpy())\n",
    "\n",
    "        w2_history.append(model[2].weight.data.clone().cpu().numpy())  \n",
    "        b2_history.append(model[2].bias.data.clone().cpu().numpy())\n",
    "\n",
    "        w3_history.append(model[4].weight.data.clone().cpu().numpy())  \n",
    "        b3_history.append(model[4].bias.data.clone().cpu().numpy())\n",
    "\n",
    "        w4_history.append(model[6].weight.data.clone().cpu().numpy())  \n",
    "        b4_history.append(model[6].bias.data.clone().cpu().numpy())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch == 1 or epoch == epochs:\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch}/{epochs} — loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Convert lists into numpy arrays for later analysis\n",
    "w1_history, b1_history = np.stack(w1_history), np.stack(b1_history)\n",
    "w2_history, b2_history = np.stack(w2_history), np.stack(b2_history)\n",
    "w3_history, b3_history = np.stack(w3_history), np.stack(b3_history)\n",
    "w4_history, b4_history = np.stack(w4_history), np.stack(b4_history)\n",
    "\n",
    "print(\"Recorded weight history shapes:\")\n",
    "print(\" Layer1 (4→16):\", w1_history.shape)\n",
    "print(\" Layer2 (16→8):\", w2_history.shape)\n",
    "print(\" Layer3 (8→4):\", w3_history.shape)\n",
    "print(\" Output  (4→3):\", w4_history.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986ea8b-e2a7-4448-8b27-76309a0796cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Evaluate Model on Training Set\n",
    "# ==============================\n",
    "\n",
    "model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
    "train_correct, train_total = 0, 0\n",
    "\n",
    "with torch.no_grad():  # No gradients needed during evaluation\n",
    "    for xb, yb in train_loader:\n",
    "        preds = model(xb).argmax(dim=1)       # Predicted class indices\n",
    "        train_correct += (preds == yb).sum().item()\n",
    "        train_total += yb.size(0)\n",
    "\n",
    "# Print training accuracy\n",
    "print(f\"Train set accuracy: {100 * train_correct / train_total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e16df-24a1-4a04-a59b-94448668641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Evaluate Model on Test Set\n",
    "# ==============================\n",
    "\n",
    "model.eval()  # Switch to evaluation mode\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient tracking\n",
    "    for xb, yb in test_loader:\n",
    "        preds = model(xb).argmax(dim=1)   # Predicted class labels\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "# Print test accuracy\n",
    "print(f\"\\nTest set accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507490aa-ee3c-418b-b833-68d2db8419bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Weight Changes in Layer 1\n",
    "# ==============================\n",
    "\n",
    "# w1_history contains all snapshots of weights for Layer 1\n",
    "# Shape: (U, 16, 4) → U updates, 16 hidden units, 4 input features\n",
    "\n",
    "# Extract the first and last snapshots\n",
    "w1_init  = w1_history[0]      # Initial weights (16 × 4)\n",
    "w1_final = w1_history[-1]     # Final weights   (16 × 4)\n",
    "\n",
    "# Compute the overall weight changes across training\n",
    "delta_w1 = w1_final - w1_init   # Shape: (16 × 4)\n",
    "\n",
    "# Create a DataFrame for better readability\n",
    "# Rows = hidden units, Columns = input features\n",
    "feature_names = ['Sepal length', 'Sepal width', 'Petal length', 'Petal width']\n",
    "hidden_units  = [f'L1 neuron {i+1}' for i in range(delta_w1.shape[0])]\n",
    "\n",
    "df_layer1_delta = pd.DataFrame(\n",
    "    delta_w1,\n",
    "    index=hidden_units,\n",
    "    columns=feature_names\n",
    ").round(3)\n",
    "\n",
    "# Display the changes in weights from initial to final\n",
    "print(\"Weight (final − initial) for Layer 1 (features → hidden1, 4→16):\")\n",
    "print(df_layer1_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba9463-afbe-417f-b561-0476cc7a4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Bias Changes in Layer 1\n",
    "# ==============================\n",
    "\n",
    "# b1_history contains all snapshots of bias for Layer 1\n",
    "\n",
    "# Extract the first and last snapshots of biases\n",
    "b1_init  = b1_history[0]      # Initial biases (16,)\n",
    "b1_final = b1_history[-1]     # Final biases   (16,)\n",
    "\n",
    "# Compute the change in biases over training\n",
    "delta_b1 = b1_final - b1_init   # Shape: (16,)\n",
    "\n",
    "# Define row labels for hidden units\n",
    "hidden_units = [f'L1 neuron {i+1}' for i in range(delta_b1.shape[0])]\n",
    "\n",
    "# Create a DataFrame with one column representing bias changes\n",
    "df_layer1_bias_delta = pd.DataFrame(\n",
    "    delta_b1.reshape(-1, 1),\n",
    "    index=hidden_units,\n",
    "    columns=['Bias']\n",
    ").round(3)\n",
    "\n",
    "# Display the changes in biases\n",
    "print(\"Bias change (final − initial) for Layer 1 (Hidden layer 1, 4→16):\")\n",
    "print(df_layer1_bias_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398307e6-c889-4bc5-9e66-fbe60c739133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Weight Changes in Layer 2\n",
    "# ==============================\n",
    "\n",
    "# Extract the first and last snapshots of weights for Layer 2\n",
    "# Shape: (8, 16) → 8 neurons in Layer 2, each connected to 16 neurons in Layer 1\n",
    "w2_init  = w2_history[0]      # Initial weights (8 × 16)\n",
    "w2_final = w2_history[-1]     # Final weights   (8 × 16)\n",
    "\n",
    "# Compute the change in weights across training\n",
    "delta_w2 = w2_final - w2_init   # Shape: (8 × 16)\n",
    "\n",
    "# Create labels:\n",
    "# - Columns = Layer 1 neurons\n",
    "# - Rows    = Layer 2 neurons\n",
    "l1_neurons = [f'L1 neuron {i+1}' for i in range(delta_w2.shape[1])]\n",
    "l2_neurons = [f'L2 neuron {i+1}' for i in range(delta_w2.shape[0])]\n",
    "\n",
    "# Build a DataFrame to display weight changes clearly\n",
    "df_layer2_delta = pd.DataFrame(\n",
    "    delta_w2,\n",
    "    index=l2_neurons,\n",
    "    columns=l1_neurons\n",
    ").round(3)\n",
    "\n",
    "# Show the changes in weights from Layer 1 → Layer 2\n",
    "print(\"Weight (final − initial) for Layer 2 (L1 → L2, 16→8):\")\n",
    "print(df_layer2_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396be4b1-d323-4462-94b5-26fda5e72194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Bias Changes in Layer 2\n",
    "# ==============================\n",
    "\n",
    "# Extract the first and last snapshots of biases for Layer 2\n",
    "# Shape: (8,) → one bias per neuron in Layer 2\n",
    "b2_init  = b2_history[0]      # Initial biases (8,)\n",
    "b2_final = b2_history[-1]     # Final biases   (8,)\n",
    "\n",
    "# Compute the change in biases across training\n",
    "delta_b2 = b2_final - b2_init   # Shape: (8,)\n",
    "\n",
    "# Define row labels for Layer 2 neurons\n",
    "l2_neurons = [f'L2 neuron {i+1}' for i in range(delta_b2.shape[0])]\n",
    "\n",
    "# Create a DataFrame with one column representing bias changes\n",
    "df_layer2_bias_delta = pd.DataFrame(\n",
    "    delta_b2.reshape(-1, 1),\n",
    "    index=l2_neurons,\n",
    "    columns=['Bias']\n",
    ").round(3)\n",
    "\n",
    "# Display the changes in biases for Layer 2\n",
    "print(\"Bias change (final − initial) for Layer 2 (Hidden layer 2, 16→8):\")\n",
    "print(df_layer2_bias_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a3fb2-ea9a-4821-a367-7f6ad0e0609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Weight Changes in Layer 3\n",
    "# ==============================\n",
    "\n",
    "# Extract the first and last snapshots of weights for Layer 3\n",
    "# Shape: (4, 8) → 4 neurons in Layer 3, each connected to 8 neurons in Layer 2\n",
    "w3_init  = w3_history[0]      # Initial weights (4 × 8)\n",
    "w3_final = w3_history[-1]     # Final weights   (4 × 8)\n",
    "\n",
    "# Compute the change in weights across training\n",
    "delta_w3 = w3_final - w3_init   # Shape: (4 × 8)\n",
    "\n",
    "# Create labels:\n",
    "# - Columns = Layer 2 neurons\n",
    "# - Rows    = Layer 3 neurons\n",
    "l2_neurons = [f'L2 neuron {i+1}' for i in range(delta_w3.shape[1])]\n",
    "l3_neurons = [f'L3 neuron {i+1}' for i in range(delta_w3.shape[0])]\n",
    "\n",
    "# Build a DataFrame to display weight changes clearly\n",
    "df_layer3_delta = pd.DataFrame(\n",
    "    delta_w3,\n",
    "    index=l3_neurons,\n",
    "    columns=l2_neurons\n",
    ").round(3)\n",
    "\n",
    "# Show the changes in weights from Layer 2 → Layer 3\n",
    "print(\"Weight (final − initial) for Layer 3 (L2 → L3, 8→4):\")\n",
    "print(df_layer3_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b55561-0d53-4dae-a850-e82b82392b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Bias Changes in Layer 3\n",
    "# ==============================\n",
    "\n",
    "# Extract the first and last snapshots of biases for Layer 3\n",
    "# Shape: (4,) → one bias per neuron in Layer 3\n",
    "b3_init  = b3_history[0]      # Initial biases (4,)\n",
    "b3_final = b3_history[-1]     # Final biases   (4,)\n",
    "\n",
    "# Compute the change in biases across training\n",
    "delta_b3 = b3_final - b3_init   # Shape: (4,)\n",
    "\n",
    "# Define row labels for Layer 3 neurons\n",
    "l3_neurons = [f'L3 neuron {i+1}' for i in range(delta_b3.shape[0])]\n",
    "\n",
    "# Create a DataFrame with one column representing bias changes\n",
    "df_layer3_bias_delta = pd.DataFrame(\n",
    "    delta_b3.reshape(-1, 1),\n",
    "    index=l3_neurons,\n",
    "    columns=['Bias']\n",
    ").round(3)\n",
    "\n",
    "# Display the changes in biases for Layer 3\n",
    "print(\"Bias change (final − initial) for Layer 3 (Hidden layer 3, 8→4):\")\n",
    "print(df_layer3_bias_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85644c76-b6ed-45a1-b16a-5c939011a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Weight Changes in Layer 4 (Output Layer)\n",
    "# ==============================\n",
    "\n",
    "# w4_history contains all snapshots of weights for the Output layer\n",
    "\n",
    "# Extract the initial and final weight snapshots\n",
    "w4_init  = w4_history[0]      # Initial weights (3 × 4)\n",
    "w4_final = w4_history[-1]     # Final weights   (3 × 4)\n",
    "\n",
    "# Compute the change in weights during training\n",
    "delta_w4 = w4_final - w4_init   # Shape: (3 × 4)\n",
    "\n",
    "# Define labels for rows and columns\n",
    "classes    = ['Setosa', 'Versicolor', 'Virginica']               # Output classes\n",
    "l3_neurons = [f'L3 neuron {i+1}' for i in range(delta_w4.shape[1])]  # Layer 3 neurons\n",
    "\n",
    "# Create a DataFrame\n",
    "# Rows = Layer 3 neurons, Columns = output classes\n",
    "df_layer4 = pd.DataFrame(\n",
    "    delta_w4.T,\n",
    "    index=l3_neurons,\n",
    "    columns=classes\n",
    ").round(3)\n",
    "\n",
    "# Display the changes in weights from Layer 3 → Output\n",
    "print(\"Weight (final − initial) for Layer 4 (L3 → Output, 4→3):\")\n",
    "print(df_layer4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1e868-ba93-4482-9854-10b943aa47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Bias Changes in Layer 4 (Output Layer)\n",
    "# ==============================\n",
    "\n",
    "# Extract the initial and final bias snapshots for Layer 4\n",
    "# Shape: (3,) → one bias per output class\n",
    "b4_init  = b4_history[0]      # Initial biases (3,)\n",
    "b4_final = b4_history[-1]     # Final biases   (3,)\n",
    "\n",
    "# Compute the change in biases during training\n",
    "delta_b4 = b4_final - b4_init   # Shape: (3,)\n",
    "\n",
    "# Define labels for output neurons (corresponding to classes)\n",
    "output_neurons = [f'Output neuron {i+1}' for i in range(delta_b4.shape[0])]\n",
    "\n",
    "# Create a DataFrame with one column showing bias changes\n",
    "df_layer4_bias_delta = pd.DataFrame(\n",
    "    delta_b4.reshape(-1, 1),\n",
    "    index=output_neurons,\n",
    "    columns=['Bias']\n",
    ").round(3)\n",
    "\n",
    "# Display the changes in biases for the output layer\n",
    "print(\"Bias change (final − initial) for Layer 4 (Output, 4→3):\")\n",
    "print(df_layer4_bias_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6946179-ce8a-4676-b026-b3d06ddb0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analytic Forward Pass Using Final Parameters (3 Hidden Layers)\n",
    "# ==============================\n",
    "\n",
    "# Final learned parameters\n",
    "w1, b1 = w1_final, b1_final     # Layer 1 (16 × 4), (16,)\n",
    "w2, b2 = w2_final, b2_final     # Layer 2 (8 × 16), (8,)\n",
    "w3, b3 = w3_final, b3_final     # Layer 3 (4 × 8), (4,)\n",
    "w4, b4 = w4_final, b4_final     # Output Layer (3 × 4), (3,)\n",
    "\n",
    "# Test data as NumPy arrays\n",
    "X_test_np = X_test.numpy()      # (N, 4)\n",
    "y_test_np = y_test.numpy()      # (N,)\n",
    "\n",
    "# Analytic forward pass (ReLU activations between linear layers)\n",
    "# Layer 1: input → hidden1\n",
    "Z1 = X_test_np.dot(w1.T) + b1   # (N, 16)\n",
    "H1 = np.maximum(0, Z1)          # ReLU\n",
    "\n",
    "# Layer 2: hidden1 → hidden2\n",
    "Z2 = H1.dot(w2.T) + b2          # (N, 8)\n",
    "H2 = np.maximum(0, Z2)          # ReLU\n",
    "\n",
    "# Layer 3: hidden2 → hidden3\n",
    "Z3 = H2.dot(w3.T) + b3          # (N, 4)\n",
    "H3 = np.maximum(0, Z3)          # ReLU\n",
    "\n",
    "# Output layer: hidden3 → output logits\n",
    "logits = H3.dot(w4.T) + b4      # (N, 3)\n",
    "y_pred2 = np.argmax(logits, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "acc = accuracy_score(y_test_np, y_pred2)\n",
    "print(f\"Analytic 3-hidden-layer net accuracy: {acc*100:.2f}%\\n\")\n",
    "print(classification_report(y_test_np, y_pred2, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45374f-d812-4252-a237-73ac180b0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Region-Specific Affine Map Function (3 Hidden Layers)\n",
    "# ==============================\n",
    "\n",
    "def compute_region_affine(x, w1, b1, w2, b2, w3, b3, w4, b4):\n",
    "    \"\"\"\n",
    "    Forward pass with ReLU masks for a 4→16→8→4→3 MLP, collapse to the region-specific affine map,\n",
    "    and return the activation-pattern region id.\n",
    "    \n",
    "    Returns:\n",
    "        A_r      : (3, 4)   Effective weight matrix for this region\n",
    "        D_r      : (3,)     Effective bias vector for this region\n",
    "        logits   : (3,)     Output logits for input x\n",
    "        pred     : int      Argmax class index\n",
    "        c1       : (16,)    L1 (16) neuron-level contributions to predicted class\n",
    "        c2       : (8,)     L2 (8) neuron-level contributions to predicted class\n",
    "        c3       : (4,)     L3 (4) neuron-level contributions to predicted class\n",
    "        region_id: str      Bitstring mask per layer, e.g., \"L1:...|L2:...|L3:...\"\n",
    "    \"\"\"\n",
    "\n",
    "    # -------- Layer 1: input (4) → hidden1 (16) --------\n",
    "    Z1 = w1.dot(x) + b1                 # (16,)\n",
    "    S1 = (Z1 > 0).astype(float)         # ReLU mask (16,)\n",
    "    H1 = np.maximum(0, Z1)              # (16,)\n",
    "\n",
    "    # -------- Layer 2: hidden1 (16) → hidden2 (8) --------\n",
    "    Z2 = w2.dot(H1) + b2                # (8,)\n",
    "    S2 = (Z2 > 0).astype(float)         # ReLU mask (8,)\n",
    "    H2 = np.maximum(0, Z2)              # (8,)\n",
    "\n",
    "    # -------- Layer 3: hidden2 (8) → hidden3 (4) --------\n",
    "    Z3 = w3.dot(H2) + b3                # (4,)\n",
    "    S3 = (Z3 > 0).astype(float)         # ReLU mask (4,)\n",
    "    H3 = np.maximum(0, Z3)              # (4,)\n",
    "\n",
    "    # -------- Output layer: hidden3 (4) → logits (3) --------\n",
    "    logits = w4.dot(H3) + b4            # (3,)\n",
    "    pred   = int(np.argmax(logits))     # scalar\n",
    "\n",
    "    # -------- Collapsed affine map (region-specific) --------\n",
    "    # Apply masks by column-scaling each weight matrix (equivalent to right-multiplying by diag(S))\n",
    "    W2m = w2 * S1[None, :]              # (8, 16)\n",
    "    W3m = w3 * S2[None, :]              # (4, 8)\n",
    "    W4m = w4 * S3[None, :]              # (3, 4)\n",
    "\n",
    "    # Effective linear map from input x (4,) to logits (3,)\n",
    "    A_r = W4m.dot(W3m.dot(W2m.dot(w1)))  # (3, 4)\n",
    "\n",
    "    # Effective bias: b4 + W4m b3 + W4m W3m b2 + W4m W3m W2m b1\n",
    "    D_r = (\n",
    "        b4\n",
    "        + W4m.dot(b3)\n",
    "        + W4m.dot(W3m.dot(b2))\n",
    "        + W4m.dot(W3m.dot(W2m.dot(b1)))\n",
    "    )  # (3,)\n",
    "\n",
    "    # -------- Neuron-level contributions to the predicted class --------\n",
    "    # Start from L3, then propagate back the masked chains for L2 and L1\n",
    "    c3 = H3 * W4m[pred]                 # (4,)\n",
    "    chain2 = W4m[pred].dot(W3m)         # (8,)\n",
    "    c2 = H2 * chain2                    # (8,)\n",
    "    chain1 = chain2.dot(W2m)            # (16,)\n",
    "    c1 = H1 * chain1                    # (16,)\n",
    "\n",
    "    # -------- Region ID (bitstrings per layer) --------\n",
    "    S1_bits = ''.join(str(int(b)) for b in S1)   # length 16\n",
    "    S2_bits = ''.join(str(int(b)) for b in S2)   # length 8\n",
    "    S3_bits = ''.join(str(int(b)) for b in S3)   # length 4\n",
    "    region_id = f\"L1:{S1_bits}|L2:{S2_bits}|L3:{S3_bits}\"\n",
    "\n",
    "    return A_r, D_r, logits, pred, c1, c2, c3, region_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64211a4-a9f3-4509-b244-9e6d21fda46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Feature Contribution Explanation Function\n",
    "# ==============================\n",
    "def explain_region(x, A_r, D_r, logits, pred, feature_names, class_names):\n",
    "    \"\"\"\n",
    "    Given affine map (A_r, D_r) for a sample, compute:\n",
    "      - Logit-level feature contributions\n",
    "      - Probability-level contributions via softmax Jacobian\n",
    "    Returns a DataFrame with contributions per feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Logit-level contributions for predicted class\n",
    "    A_c = A_r[pred]                  # Row of A_r for predicted class\n",
    "    f_logit = A_c * x                # Feature × coefficient\n",
    "\n",
    "    # Compute class probabilities\n",
    "    logits_r = A_r.dot(x) + D_r\n",
    "    probs = softmax(logits_r)\n",
    "\n",
    "    # Softmax Jacobian for predicted class\n",
    "    J = -probs[pred] * probs\n",
    "    J[pred] = probs[pred] * (1 - probs[pred])\n",
    "\n",
    "    # Probability-level contributions\n",
    "    f_all = A_r * x\n",
    "    f_prob = J.dot(f_all)\n",
    "\n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'LogitContribution': f_logit,\n",
    "        'ProbContribution': f_prob,\n",
    "        'PredProbability': probs[pred]\n",
    "    }, index=feature_names).round(4)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f91f6-bedd-493b-a274-71b52b4b5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Use final parameters (3 hidden layers)\n",
    "# ==============================\n",
    "w1, b1 = w1_final, b1_final     # Layer 1 (16 × 4), (16,)\n",
    "w2, b2 = w2_final, b2_final     # Layer 2 (8  × 16), (8,)\n",
    "w3, b3 = w3_final, b3_final     # Layer 3 (4  × 8),  (4,)\n",
    "w4, b4 = w4_final, b4_final     # Output  (3  × 4),  (3,)\n",
    "\n",
    "# ==============================\n",
    "# Prepare data & names\n",
    "# ==============================\n",
    "feature_names = ['Sepal length', 'Sepal width', 'Petal length', 'Petal width']\n",
    "class_names   = le.classes_.tolist()\n",
    "\n",
    "# ==============================\n",
    "# TRAIN set: explanations, neuron contributions, and regions\n",
    "# ==============================\n",
    "X_train_np = X_train.numpy()                 # (N_train, 4)\n",
    "N_train, _ = X_train_np.shape\n",
    "\n",
    "all_explanations_train = []                  # per-sample feature contributions\n",
    "neuron1_train = []                           # L1 (16,) neuron contributions per sample\n",
    "neuron2_train = []                           # L2 (8,)  neuron contributions per sample\n",
    "neuron3_train = []                           # L3 (4,)  neuron contributions per sample\n",
    "rank_counts_train = {f: [0]*len(feature_names) for f in feature_names}\n",
    "y_train_preds = np.zeros(N_train, dtype=int) # predictions\n",
    "train_regions = {}                           # region_id -> list of train indices\n",
    "\n",
    "for i in range(N_train):\n",
    "    x = X_train_np[i]\n",
    "\n",
    "    # Collapse to region-specific affine and get region id\n",
    "    A_r, D_r, logits, pred, c1, c2, c3, region_id = compute_region_affine(\n",
    "        x, w1, b1, w2, b2, w3, b3, w4, b4\n",
    "    )\n",
    "    y_train_preds[i] = pred\n",
    "\n",
    "    # Feature-level explanation\n",
    "    df_exp = explain_region(x, A_r, D_r, logits, pred, feature_names, class_names)\n",
    "    df_exp.insert(0, 'Sample', i)\n",
    "    all_explanations_train.append(df_exp)\n",
    "\n",
    "    # Rank features by absolute probability contribution\n",
    "    ranking = np.argsort(-df_exp['ProbContribution'].abs().values)\n",
    "    for rank, feat_idx in enumerate(ranking):\n",
    "        rank_counts_train[feature_names[feat_idx]][rank] += 1\n",
    "\n",
    "    # Store neuron contributions\n",
    "    neuron1_train.append(c1)    # shape (16,)\n",
    "    neuron2_train.append(c2)    # shape (8,)\n",
    "    neuron3_train.append(c3)    # shape (4,)\n",
    "\n",
    "    # Accumulate region membership\n",
    "    train_regions.setdefault(region_id, []).append(i)\n",
    "\n",
    "# Tidy outputs (optional tables)\n",
    "big_df_train = (\n",
    "    pd.concat(all_explanations_train)\n",
    "      .reset_index().rename(columns={'index': 'Feature'})\n",
    "      .set_index(['Sample','Feature'])\n",
    ")\n",
    "neuron1_train = np.stack(neuron1_train)      # (N_train, 16)\n",
    "neuron2_train = np.stack(neuron2_train)      # (N_train, 8)\n",
    "neuron3_train = np.stack(neuron3_train)      # (N_train, 4)\n",
    "rank_df_train = pd.DataFrame(\n",
    "    rank_counts_train,\n",
    "    index=[f'Rank {r+1}' for r in range(len(feature_names))]\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# TEST set: explanations, neuron contributions, and regions\n",
    "# ==============================\n",
    "X_test_np = X_test.numpy()                   # (N_test, 4)\n",
    "N_test, _ = X_test_np.shape\n",
    "\n",
    "all_explanations_test = []\n",
    "neuron1_test = []\n",
    "neuron2_test = []\n",
    "neuron3_test = []\n",
    "rank_counts_test = {f: [0]*len(feature_names) for f in feature_names}\n",
    "y_test_preds = np.zeros(N_test, dtype=int)\n",
    "test_regions = {}                            # region_id -> list of test indices\n",
    "\n",
    "for i in range(N_test):\n",
    "    x = X_test_np[i]\n",
    "\n",
    "    # Collapse to region-specific affine and get region id\n",
    "    A_r, D_r, logits, pred, c1, c2, c3, region_id = compute_region_affine(\n",
    "        x, w1, b1, w2, b2, w3, b3, w4, b4\n",
    "    )\n",
    "    y_test_preds[i] = pred\n",
    "\n",
    "    # Feature-level explanation\n",
    "    df_exp = explain_region(x, A_r, D_r, logits, pred, feature_names, class_names)\n",
    "    df_exp.insert(0, 'Sample', i)\n",
    "    all_explanations_test.append(df_exp)\n",
    "\n",
    "    # Rank features by absolute probability contribution\n",
    "    ranking = np.argsort(-df_exp['ProbContribution'].abs().values)\n",
    "    for rank, feat_idx in enumerate(ranking):\n",
    "        rank_counts_test[feature_names[feat_idx]][rank] += 1\n",
    "\n",
    "    # Store neuron contributions\n",
    "    neuron1_test.append(c1)                 # (16,)\n",
    "    neuron2_test.append(c2)                 # (8,)\n",
    "    neuron3_test.append(c3)                 # (4,)\n",
    "\n",
    "    # Accumulate region membership\n",
    "    test_regions.setdefault(region_id, []).append(i)\n",
    "\n",
    "# Tidy outputs (optional tables)\n",
    "big_df_test = (\n",
    "    pd.concat(all_explanations_test)\n",
    "      .reset_index().rename(columns={'index': 'Feature'})\n",
    "      .set_index(['Sample','Feature'])\n",
    ")\n",
    "neuron1_test = np.stack(neuron1_test)        # (N_test, 16)\n",
    "neuron2_test = np.stack(neuron2_test)        # (N_test, 8)\n",
    "neuron3_test = np.stack(neuron3_test)        # (N_test, 4)\n",
    "rank_df_test = pd.DataFrame(\n",
    "    rank_counts_test,\n",
    "    index=[f'Rank {r+1}' for r in range(len(feature_names))]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65779fc2-a107-40fb-b09e-993bcaa75938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Region analysis: overlap, purity, coverage, feature profiles (TEST set) — 3 hidden layers\n",
    "# ==============================\n",
    "\n",
    "def region_analysis(\n",
    "    train_regions,             # dict: region_id -> list(train idx)\n",
    "    test_regions,              # dict: region_id -> list(test idx)\n",
    "    X_test_np,                 # (N_test, d)\n",
    "    y_test_np,                 # (N_test,)\n",
    "    y_test_preds,              # (N_test,)\n",
    "    feature_names,             # list[str], length d\n",
    "    class_names,               # list[str], length C\n",
    "    w1, b1, w2, b2, w3, b3, w4, b4,  # final parameters (4→16→8→4→3)\n",
    "    purity_threshold=0.7\n",
    "):\n",
    "    # Overlap counts\n",
    "    train_ids = set(train_regions.keys())\n",
    "    test_ids  = set(test_regions.keys())\n",
    "    n_regions_train = len(train_ids)\n",
    "    n_regions_test  = len(test_ids)\n",
    "    overlap_count   = len(train_ids & test_ids)\n",
    "    test_only_count = len(test_ids - train_ids)\n",
    "    overlap_pct     = overlap_count / max(1, n_regions_test)\n",
    "\n",
    "    # Per-region class purity (TEST) uses ground truth\n",
    "    C = len(class_names)\n",
    "    rows = []\n",
    "    for region_id, idxs in test_regions.items():\n",
    "        if not idxs:\n",
    "            continue\n",
    "        labels = y_test_np[idxs]                      # ground-truth labels\n",
    "        counts = np.bincount(labels, minlength=C)     # class histogram\n",
    "        n_r = len(idxs)\n",
    "        purity = (counts.max() / n_r) if n_r > 0 else 0.0\n",
    "        is_confusion = (purity < purity_threshold)\n",
    "\n",
    "        row = {'region_id': region_id, 'n_r': n_r, 'purity': purity, 'is_confusion': is_confusion}\n",
    "        for c in range(C):\n",
    "            row[f'count_{class_names[c]}'] = int(counts[c])\n",
    "        rows.append(row)\n",
    "\n",
    "    region_summary_test = (\n",
    "        pd.DataFrame(rows)\n",
    "          .set_index('region_id')\n",
    "          .sort_values(by='n_r', ascending=False)\n",
    "    )\n",
    "\n",
    "    # Coverage metrics (TEST)\n",
    "    N_test = len(y_test_np)\n",
    "    region_sizes = sorted((len(v) for v in test_regions.values()), reverse=True)\n",
    "    top1_coverage = (region_sizes[0] / N_test) if region_sizes else 0.0\n",
    "    topC_coverage = (sum(region_sizes[:C]) / N_test) if region_sizes else 0.0\n",
    "\n",
    "    # Region-wise feature profiles (TEST)\n",
    "    majority_rows = []\n",
    "    per_class_rows = []\n",
    "    mixture_rows = []\n",
    "\n",
    "    for region_id, idxs in test_regions.items():\n",
    "        if not idxs:\n",
    "            continue\n",
    "\n",
    "        # Region affine map (any sample from region; masks define A_r, D_r)\n",
    "        x0 = X_test_np[idxs[0]]\n",
    "        A_r, D_r, _, _, _, _, _, _ = compute_region_affine(x0, w1, b1, w2, b2, w3, b3, w4, b4)\n",
    "\n",
    "        # ---------- Majority class by GROUND TRUTH ----------\n",
    "        labels_region = y_test_np[idxs]\n",
    "        cls_idx = int(np.bincount(labels_region, minlength=C).argmax())\n",
    "        cls_name = class_names[cls_idx]\n",
    "\n",
    "        # Majority profile (use A_r[cls_idx], average over ALL samples in region)\n",
    "        X_reg = X_test_np[idxs]                                  # (n_r, d)\n",
    "        majority_contrib = (A_r[cls_idx][None, :] * X_reg).mean(axis=0)\n",
    "        maj_row = {'region_id': region_id, 'class_used': cls_name}\n",
    "        for j, feat in enumerate(feature_names):\n",
    "            maj_row[feat] = float(np.round(majority_contrib[j], 4))\n",
    "        majority_rows.append(maj_row)\n",
    "\n",
    "        # ---------- Per-class profiles by GROUND TRUTH ----------\n",
    "        for c in range(C):\n",
    "            idxs_c = [k for k in idxs if y_test_np[k] == c]\n",
    "            if not idxs_c:\n",
    "                continue\n",
    "            X_reg_c = X_test_np[idxs_c]                           # (n_rc, d)\n",
    "            pc_contrib = (A_r[c][None, :] * X_reg_c).mean(axis=0)\n",
    "            pc_row = {'region_id': region_id, 'class_used': class_names[c]}\n",
    "            for j, feat in enumerate(feature_names):\n",
    "                pc_row[feat] = float(np.round(pc_contrib[j], 4))\n",
    "            per_class_rows.append(pc_row)\n",
    "\n",
    "        # Mixture profile (per PREDICTED class behavior)\n",
    "        mix_contribs = []\n",
    "        for k in idxs:\n",
    "            c_pred = int(y_test_preds[k])\n",
    "            mix_contribs.append(A_r[c_pred] * X_test_np[k])\n",
    "        mix_profile = np.mean(np.stack(mix_contribs, axis=0), axis=0)\n",
    "        mix_row = {'region_id': region_id, 'class_used': 'mixture'}\n",
    "        for j, feat in enumerate(feature_names):\n",
    "            mix_row[feat] = float(np.round(mix_profile[j], 4))\n",
    "        mixture_rows.append(mix_row)\n",
    "\n",
    "    # Build DataFrames\n",
    "    region_feature_profile_test = (\n",
    "        pd.DataFrame(majority_rows)\n",
    "          .set_index('region_id')\n",
    "          .sort_index()\n",
    "    )\n",
    "    region_feature_profile_test_per_class = (\n",
    "        pd.DataFrame(per_class_rows)\n",
    "          .set_index(['region_id', 'class_used'])\n",
    "          .sort_index()\n",
    "    )\n",
    "    region_feature_profile_test_mixture = (\n",
    "        pd.DataFrame(mixture_rows)\n",
    "          .set_index('region_id')\n",
    "          .sort_index()\n",
    "    )\n",
    "\n",
    "    # Outputs\n",
    "    return {\n",
    "        'n_regions_train': n_regions_train,\n",
    "        'n_regions_test' : n_regions_test,\n",
    "        'overlap_count'  : overlap_count,\n",
    "        'test_only_count': test_only_count,\n",
    "        'overlap_pct'    : overlap_pct,\n",
    "        'region_summary_test'                : region_summary_test,\n",
    "        'top1_coverage'                      : top1_coverage,\n",
    "        'topC_coverage'                      : topC_coverage,\n",
    "        'region_feature_profile_test'        : region_feature_profile_test,              # majority (by GT)\n",
    "        'region_feature_profile_test_per_class': region_feature_profile_test_per_class,  # per-class (by GT)\n",
    "        'region_feature_profile_test_mixture': region_feature_profile_test_mixture       # mixture (by pred)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04502105-dde5-4ac1-b4ea-a58ada12169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Example usage (3 hidden layers)\n",
    "# ==============================\n",
    "\n",
    "# Final parameters\n",
    "w1, b1 = w1_final, b1_final\n",
    "w2, b2 = w2_final, b2_final\n",
    "w3, b3 = w3_final, b3_final\n",
    "w4, b4 = w4_final, b4_final\n",
    "\n",
    "# Arrays\n",
    "X_test_np = X_test.numpy()\n",
    "y_test_np = y_test.numpy()\n",
    "\n",
    "# Run analysis\n",
    "results = region_analysis(\n",
    "    train_regions=train_regions,\n",
    "    test_regions=test_regions,\n",
    "    X_test_np=X_test_np,\n",
    "    y_test_np=y_test_np,\n",
    "    y_test_preds=y_test_preds,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    w1=w1, b1=b1, w2=w2, b2=b2, w3=w3, b3=b3, w4=w4, b4=b4,\n",
    "    purity_threshold=0.7\n",
    ")\n",
    "\n",
    "# Quick prints\n",
    "print(f\"Train regions: {results['n_regions_train']}\")\n",
    "print(f\"Test regions: {results['n_regions_test']}\")\n",
    "print(f\"Overlap (count): {results['overlap_count']}\")\n",
    "print(f\"Test-only regions: {results['test_only_count']}\")\n",
    "print(f\"Overlap (% of test regions): {results['overlap_pct']:.2%}\")\n",
    "print(f\"Top-1 coverage (test): {results['top1_coverage']:.2%}\")\n",
    "print(f\"Top-{len(class_names)} coverage (test): {results['topC_coverage']:.2%}\")\n",
    "\n",
    "print(\"\\n=== Region summary (test) ===\")\n",
    "print(results['region_summary_test'].head(20))\n",
    "\n",
    "print(\"\\n=== Region-wise feature profile (majority class, test) ===\")\n",
    "print(results['region_feature_profile_test'].head(20))\n",
    "\n",
    "print(\"\\n=== Region-wise feature profile (per-class, test) ===\")\n",
    "print(results['region_feature_profile_test_per_class'].head(20))\n",
    "\n",
    "print(\"\\n=== Region-wise feature profile (mixture, test) ===\")\n",
    "print(results['region_feature_profile_test_mixture'].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6666e-6a71-4c5d-ac8f-f4e34f23771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Super-Neuron Identification — Two Hidden Layers\n",
    "# ==========================================\n",
    "\n",
    "neuron1_matrix = neuron1_train\n",
    "neuron2_matrix = neuron2_train\n",
    "neuron3_matrix = neuron3_train\n",
    "\n",
    "for top_k in [1, 2, 3]:\n",
    "    freq1 = np.zeros(neuron1_matrix.shape[1], dtype=int)\n",
    "    freq2 = np.zeros(neuron2_matrix.shape[1], dtype=int)\n",
    "    freq3 = np.zeros(neuron3_matrix.shape[1], dtype=int)\n",
    "\n",
    "    for c1, c2, c3 in zip(neuron1_matrix, neuron2_matrix, neuron3_matrix):\n",
    "        # Rank neurons by absolute contribution magnitude\n",
    "        top1 = np.argsort(-np.abs(c1))[:top_k]\n",
    "        top2 = np.argsort(-np.abs(c2))[:top_k]\n",
    "        top3 = np.argsort(-np.abs(c3))[:top_k]\n",
    "        freq1[top1] += 1\n",
    "        freq2[top2] += 1\n",
    "        freq3[top3] += 1\n",
    "\n",
    "    super1 = int(np.argmax(freq1))\n",
    "    super2 = int(np.argmax(freq2))\n",
    "    super3 = int(np.argmax(freq3))\n",
    "\n",
    "    print(f\"Top-{top_k} super-neuron (Layer 1): #{super1} \"\n",
    "          f\"(appears in top-{top_k} for {freq1[super1]} samples)\")\n",
    "    print(f\"Top-{top_k} super-neuron (Layer 2): #{super2} \"\n",
    "          f\"(appears in top-{top_k} for {freq2[super2]} samples)\")\n",
    "    print(f\"Top-{top_k} super-neuron (Layer 3): #{super3} \"\n",
    "          f\"(appears in top-{top_k} for {freq3[super3]} samples)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "#check neuron activity\n",
    "active1 = np.sum(np.abs(neuron1_train) > 1e-6, axis=0) > 0\n",
    "active2 = np.sum(np.abs(neuron2_train) > 1e-6, axis=0) > 0\n",
    "active3 = np.sum(np.abs(neuron3_train) > 1e-6, axis=0) > 0\n",
    "\n",
    "print(f\"Layer 1: {active1.sum()}/{len(active1)} neurons active, \"\n",
    "      f\"{(~active1).sum()} inactive\")\n",
    "print(f\"Layer 2: {active2.sum()}/{len(active2)} neurons active, \"\n",
    "      f\"{(~active2).sum()} inactive\")\n",
    "print(f\"Layer 3: {active3.sum()}/{len(active3)} neurons active, \"\n",
    "      f\"{(~active3).sum()} inactive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c931ce-a046-413d-8c14-d677bfb08a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Super-Neuron Identification — Two Hidden Layers\n",
    "# ==========================================\n",
    "\n",
    "neuron1_matrix = neuron1_test\n",
    "neuron2_matrix = neuron2_test\n",
    "neuron3_matrix = neuron3_test\n",
    "\n",
    "for top_k in [1, 2, 3]:\n",
    "    freq1 = np.zeros(neuron1_matrix.shape[1], dtype=int)\n",
    "    freq2 = np.zeros(neuron2_matrix.shape[1], dtype=int)\n",
    "    freq3 = np.zeros(neuron3_matrix.shape[1], dtype=int)\n",
    "\n",
    "    for c1, c2, c3 in zip(neuron1_matrix, neuron2_matrix, neuron3_matrix):\n",
    "        # Rank neurons by absolute contribution magnitude\n",
    "        top1 = np.argsort(-np.abs(c1))[:top_k]\n",
    "        top2 = np.argsort(-np.abs(c2))[:top_k]\n",
    "        top3 = np.argsort(-np.abs(c3))[:top_k]\n",
    "        freq1[top1] += 1\n",
    "        freq2[top2] += 1\n",
    "        freq3[top3] += 1\n",
    "\n",
    "    super1 = int(np.argmax(freq1))\n",
    "    super2 = int(np.argmax(freq2))\n",
    "    super3 = int(np.argmax(freq3))\n",
    "\n",
    "    print(f\"Top-{top_k} super-neuron (Layer 1): #{super1} \"\n",
    "          f\"(appears in top-{top_k} for {freq1[super1]} samples)\")\n",
    "    print(f\"Top-{top_k} super-neuron (Layer 2): #{super2} \"\n",
    "          f\"(appears in top-{top_k} for {freq2[super2]} samples)\")\n",
    "    print(f\"Top-{top_k} super-neuron (Layer 3): #{super3} \"\n",
    "          f\"(appears in top-{top_k} for {freq3[super3]} samples)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "#check neuron activity\n",
    "active1 = np.sum(np.abs(neuron1_test) > 1e-6, axis=0) > 0\n",
    "active2 = np.sum(np.abs(neuron2_test) > 1e-6, axis=0) > 0\n",
    "active3 = np.sum(np.abs(neuron3_test) > 1e-6, axis=0) > 0\n",
    "\n",
    "print(f\"Layer 1: {active1.sum()}/{len(active1)} neurons active, \"\n",
    "      f\"{(~active1).sum()} inactive\")\n",
    "print(f\"Layer 2: {active2.sum()}/{len(active2)} neurons active, \"\n",
    "      f\"{(~active2).sum()} inactive\")\n",
    "print(f\"Layer 3: {active3.sum()}/{len(active3)} neurons active, \"\n",
    "      f\"{(~active3).sum()} inactive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c376d-af25-40f5-afc7-9e9745da483d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
