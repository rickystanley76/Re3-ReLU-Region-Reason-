{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed6972-3ace-4781-a936-2117224eed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Import Required Libraries\n",
    "# ==============================\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# PyTorch core modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, ndcg_score\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# Explainability libraries\n",
    "from lime import lime_tabular\n",
    "import shap\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f0cd4-a2dc-4a13-a983-70e9d237f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Load Dataset\n",
    "# ==============================\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "# Note: Ensure that file is located in the working directory\n",
    "df = pd.read_csv('iris.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793490b0-5779-4099-9d80-6f5331fe8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Encode Labels and Split Features/Targets\n",
    "# ==============================\n",
    "\n",
    "# Initialise a LabelEncoder to convert string labels (e.g., 'setosa', 'versicolor', 'virginica')\n",
    "# into numerical labels (0, 1, 2).\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Transform the last column (species names) into numeric values and store in a new column 'label'\n",
    "df['label'] = le.fit_transform(df.iloc[:, -1])\n",
    "\n",
    "# Drop the original string label column (second-to-last column) after encoding\n",
    "df = df.drop(columns=[df.columns[-2]])\n",
    "\n",
    "# Define features (X) and labels (y)\n",
    "X = df.iloc[:, :-1].values   # Select all columns except 'label' → feature matrix\n",
    "y = df['label'].values       # Select the 'label' column → target vector (0,1,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c618cadb-22ec-4f47-8fc1-918f6d5cb0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Train/Test Split and Data Preparation\n",
    "# ==============================\n",
    "\n",
    "# Split the dataset into training and test sets (80% / 20%).\n",
    "# 'stratify=y' ensures class distribution is preserved in both sets.\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Initialise a StandardScaler and fit it on the training features.\n",
    "# This standardises features (zero mean, unit variance) to improve training stability.\n",
    "scaler = StandardScaler().fit(X_train_raw)\n",
    "\n",
    "# Apply the fitted scaler to both training and test features\n",
    "X_train = scaler.transform(X_train_raw)\n",
    "X_test  = scaler.transform(X_test_raw)\n",
    "\n",
    "# Convert numpy arrays into PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)   # Features → float32\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)      # Labels → integers (long)\n",
    "X_test  = torch.tensor(X_test,  dtype=torch.float32)\n",
    "y_test  = torch.tensor(y_test,  dtype=torch.long)\n",
    "\n",
    "# Wrap tensors into TensorDataset objects and create DataLoaders\n",
    "# - train_loader: batches of training data, shuffled\n",
    "# - test_loader: batches of test data, not shuffled\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
    "test_loader  = DataLoader(TensorDataset(X_test,  y_test),  batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ccdb5f-6d12-486a-a724-1dbacb536255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Define Neural Network Model\n",
    "# ==============================\n",
    "\n",
    "# Create a feedforward neural network using nn.Sequential\n",
    "# - Input layer: 4 features (Iris measurements)\n",
    "# - Hidden layers: two fully connected (Linear) layers with ReLU activations\n",
    "# - Output layer: 3 neurons (one for each Iris class)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 8),   # Input → Hidden layer 1 (4 inputs → 8 hidden units)\n",
    "    nn.ReLU(),         # Activation function\n",
    "    nn.Linear(8, 4),   # Hidden layer 1 → Hidden layer 2 (8 → 4 units)\n",
    "    nn.ReLU(),         # Activation function\n",
    "    nn.Linear(4, 3)    # Hidden layer 2 → Output layer (4 → 3 classes)\n",
    ")\n",
    "\n",
    "# Display the architecture of the model\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74925b84-0a2d-4781-a162-a38219dec76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Training Loop with Weight History Tracking\n",
    "# ==============================\n",
    "\n",
    "# Lists to store weight/bias snapshots for each layer\n",
    "w1_history, b1_history = [], []\n",
    "w2_history, b2_history = [], []\n",
    "w3_history, b3_history = [], []\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record weights/biases after each update\n",
    "        w1_history.append(model[0].weight.data.clone().cpu().numpy())  \n",
    "        b1_history.append(model[0].bias.data.clone().cpu().numpy())\n",
    "        w2_history.append(model[2].weight.data.clone().cpu().numpy())  \n",
    "        b2_history.append(model[2].bias.data.clone().cpu().numpy())\n",
    "        w3_history.append(model[4].weight.data.clone().cpu().numpy())  \n",
    "        b3_history.append(model[4].bias.data.clone().cpu().numpy())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    if epoch == 1 or epoch == epochs:\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch}/{epochs} — loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Convert lists into numpy arrays for later analysis\n",
    "w1_history, b1_history = np.stack(w1_history), np.stack(b1_history)\n",
    "w2_history, b2_history = np.stack(w2_history), np.stack(b2_history)\n",
    "w3_history, b3_history = np.stack(w3_history), np.stack(b3_history)\n",
    "\n",
    "print(\"Recorded weight history shapes:\")\n",
    "print(\" Layer1 (4→8):\", w1_history.shape)\n",
    "print(\" Layer2 (8→4):\", w2_history.shape)\n",
    "print(\" Layer3 (4→3):\", w3_history.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986ea8b-e2a7-4448-8b27-76309a0796cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Evaluate Model on Training Set\n",
    "# ==============================\n",
    "\n",
    "model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
    "train_correct, train_total = 0, 0\n",
    "\n",
    "with torch.no_grad():  # No gradients needed during evaluation\n",
    "    for xb, yb in train_loader:\n",
    "        preds = model(xb).argmax(dim=1)       # Predicted class indices\n",
    "        train_correct += (preds == yb).sum().item()\n",
    "        train_total += yb.size(0)\n",
    "\n",
    "# Print training accuracy\n",
    "print(f\"Train set accuracy: {100 * train_correct / train_total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e16df-24a1-4a04-a59b-94448668641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Evaluate Model on Test Set\n",
    "# ==============================\n",
    "\n",
    "model.eval()  # Switch to evaluation mode\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient tracking\n",
    "    for xb, yb in test_loader:\n",
    "        preds = model(xb).argmax(dim=1)   # Predicted class labels\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "# Print test accuracy\n",
    "print(f\"\\nTest set accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507490aa-ee3c-418b-b833-68d2db8419bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Weight Changes in Layer 1\n",
    "# ==============================\n",
    "\n",
    "# w1_history contains all snapshots of weights for Layer 1\n",
    "# Shape: (80, 8, 4) → 80 updates, 8 hidden units, 4 input features\n",
    "\n",
    "# Extract the first and last snapshots\n",
    "w1_init  = w1_history[0]      # Initial weights (8 × 4)\n",
    "w1_final = w1_history[-1]     # Final weights   (8 × 4)\n",
    "\n",
    "# Compute the overall weight changes across training\n",
    "delta_w1 = w1_final - w1_init   # Shape: (8 × 4)\n",
    "\n",
    "# Create a DataFrame for better readability\n",
    "# Rows = hidden units, Columns = input features\n",
    "feature_names = ['Sepal length', 'Sepal width', 'Petal length', 'Petal width']\n",
    "hidden_units  = [f'L1 neuron {i+1}' for i in range(delta_w1.shape[0])]\n",
    "\n",
    "df_layer1_delta = pd.DataFrame(\n",
    "    delta_w1,\n",
    "    index=hidden_units,\n",
    "    columns=feature_names\n",
    ").round(3)\n",
    "\n",
    "# Display the changes in weights from initial to final\n",
    "print(\"Weight (final − initial) for Layer 1 (features → hidden):\")\n",
    "print(df_layer1_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba9463-afbe-417f-b561-0476cc7a4b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Bias Changes in Layer 1\n",
    "# ==============================\n",
    "\n",
    "# b1_history contains all snapshots of bias for Layer 1\n",
    "\n",
    "# Extract the first and last snapshots of biases\n",
    "b1_init  = b1_history[0]      # Initial biases (8,)\n",
    "b1_final = b1_history[-1]     # Final biases   (8,)\n",
    "\n",
    "# Compute the change in biases over training\n",
    "delta_b1 = b1_final - b1_init   # Shape: (8,)\n",
    "\n",
    "# Define row labels for hidden units\n",
    "hidden_units = [f'L1 neuron {i+1}' for i in range(delta_b1.shape[0])]\n",
    "\n",
    "# Create a DataFrame with one column representing bias changes\n",
    "df_layer1_bias_delta = pd.DataFrame(\n",
    "    delta_b1.reshape(-1, 1),\n",
    "    index=hidden_units,\n",
    "    columns=['Bias']\n",
    ").round(3)\n",
    "\n",
    "# Display the changes in biases\n",
    "print(\"Bias change (final − initial) for Layer 1:\")\n",
    "print(df_layer1_bias_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398307e6-c889-4bc5-9e66-fbe60c739133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Weight Changes in Layer 2\n",
    "# ==============================\n",
    "\n",
    "# Extract the first and last snapshots of weights for Layer 2\n",
    "# Shape: (4, 8) → 4 neurons in Layer 2, each connected to 8 neurons in Layer 1\n",
    "w2_init  = w2_history[0]      # Initial weights (4 × 8)\n",
    "w2_final = w2_history[-1]     # Final weights   (4 × 8)\n",
    "\n",
    "# Compute the change in weights across training\n",
    "delta_w2 = w2_final - w2_init   # Shape: (4 × 8)\n",
    "\n",
    "# Create labels:\n",
    "# - Columns = Layer 1 neurons\n",
    "# - Rows    = Layer 2 neurons\n",
    "l1_neurons = [f'L1 neuron {i+1}' for i in range(delta_w2.shape[1])]\n",
    "l2_neurons = [f'L2 neuron {i+1}' for i in range(delta_w2.shape[0])]\n",
    "\n",
    "# Build a DataFrame to display weight changes clearly\n",
    "df_layer2_delta = pd.DataFrame(\n",
    "    delta_w2,\n",
    "    index=l2_neurons,\n",
    "    columns=l1_neurons\n",
    ").round(3)\n",
    "\n",
    "# Show the changes in weights from Layer 1 → Layer 2\n",
    "print(\"Weight (final − initial) for Layer 2 (L1 → L2):\")\n",
    "print(df_layer2_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396be4b1-d323-4462-94b5-26fda5e72194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Bias Changes in Layer 2\n",
    "# ==============================\n",
    "\n",
    "# Extract the first and last snapshots of biases for Layer 2\n",
    "# Shape: (4,) → one bias per neuron in Layer 2\n",
    "b2_init  = b2_history[0]      # Initial biases (4,)\n",
    "b2_final = b2_history[-1]     # Final biases   (4,)\n",
    "\n",
    "# Compute the change in biases across training\n",
    "delta_b2 = b2_final - b2_init   # Shape: (4,)\n",
    "\n",
    "# Define row labels for Layer 2 neurons\n",
    "l2_neurons = [f'L2 neuron {i+1}' for i in range(delta_b2.shape[0])]\n",
    "\n",
    "# Create a DataFrame with one column representing bias changes\n",
    "df_layer2_bias_delta = pd.DataFrame(\n",
    "    delta_b2.reshape(-1, 1),\n",
    "    index=l2_neurons,\n",
    "    columns=['Bias']\n",
    ").round(3)\n",
    "\n",
    "# Display the changes in biases for Layer 2\n",
    "print(\"Bias change (final − initial) for Layer 2:\")\n",
    "print(df_layer2_bias_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85644c76-b6ed-45a1-b16a-5c939011a6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Weight Changes in Layer 3 (Output Layer)\n",
    "# ==============================\n",
    "\n",
    "# w3_history contains all snapshots of weights for the Output layer\n",
    "\n",
    "# Extract the initial and final weight snapshots\n",
    "w3_init  = w3_history[0]      # Initial weights (3 × 4)\n",
    "w3_final = w3_history[-1]     # Final weights   (3 × 4)\n",
    "\n",
    "# Compute the change in weights during training\n",
    "delta_w3 = w3_final - w3_init   # Shape: (3 × 4)\n",
    "\n",
    "# Define labels for rows and columns\n",
    "classes    = ['Setosa', 'Versicolor', 'Virginica']               # Output classes\n",
    "l2_neurons = [f'L2 neuron {i+1}' for i in range(delta_w3.shape[1])]  # Layer 2 neurons\n",
    "\n",
    "# Create a DataFrame\n",
    "# Rows = Layer 2 neurons, Columns = output classes\n",
    "df_layer3 = pd.DataFrame(\n",
    "    delta_w3.T,\n",
    "    index=l2_neurons,\n",
    "    columns=classes\n",
    ").round(3)\n",
    "\n",
    "# Display the changes in weights from Layer 2 → Output\n",
    "print(\"Weight (final − initial) for Layer 3 (L2 → Output):\")\n",
    "print(df_layer3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1e868-ba93-4482-9854-10b943aa47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analyse Bias Changes in Layer 3 (Output Layer)\n",
    "# ==============================\n",
    "\n",
    "# Extract the initial and final bias snapshots for Layer 3\n",
    "# Shape: (3,) → one bias per output class\n",
    "b3_init  = b3_history[0]      # Initial biases (3,)\n",
    "b3_final = b3_history[-1]     # Final biases   (3,)\n",
    "\n",
    "# Compute the change in biases during training\n",
    "delta_b3 = b3_final - b3_init   # Shape: (3,)\n",
    "\n",
    "# Define labels for output neurons (corresponding to classes)\n",
    "output_neurons = [f'Output neuron {i+1}' for i in range(delta_b3.shape[0])]\n",
    "\n",
    "# Create a DataFrame with one column showing bias changes\n",
    "df_layer3_bias_delta = pd.DataFrame(\n",
    "    delta_b3.reshape(-1, 1),\n",
    "    index=output_neurons,\n",
    "    columns=['Bias']\n",
    ").round(3)\n",
    "\n",
    "# Display the changes in biases for the output layer\n",
    "print(\"Bias change (final − initial) for Layer 3 (Output):\")\n",
    "print(df_layer3_bias_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6946179-ce8a-4676-b026-b3d06ddb0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Analytic Forward Pass Using Final Parameters\n",
    "# ==============================\n",
    "\n",
    "# Final learned parameters\n",
    "w1, b1 = w1_final, b1_final     # Layer 1 (8 × 4), (8,)\n",
    "w2, b2 = w2_final, b2_final     # Layer 2 (4 × 8), (4,)\n",
    "w3, b3 = w3_final, b3_final     # Layer 3 (3 × 4), (3,)\n",
    "\n",
    "# Test data as NumPy arrays\n",
    "X_test_np = X_test.numpy()      # (N, 4)\n",
    "y_test_np = y_test.numpy()      # (N,)\n",
    "\n",
    "# Analytic forward pass (ReLU activations between linear layers)\n",
    "# Layer 1: input → hidden1\n",
    "Z1 = X_test_np.dot(w1.T) + b1   # (N, 8)\n",
    "H1 = np.maximum(0, Z1)          # ReLU\n",
    "\n",
    "# Layer 2: hidden1 → hidden2\n",
    "Z2 = H1.dot(w2.T) + b2          # (N, 4)\n",
    "H2 = np.maximum(0, Z2)          # ReLU\n",
    "\n",
    "# Layer 3: hidden2 → output logits\n",
    "logits = H2.dot(w3.T) + b3      # (N, 3)\n",
    "y_pred2 = np.argmax(logits, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "acc = accuracy_score(y_test_np, y_pred2)\n",
    "print(f\"Analytic 2-hidden-layer net accuracy: {acc*100:.2f}%\\n\")\n",
    "print(classification_report(y_test_np, y_pred2, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45374f-d812-4252-a237-73ac180b0395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Region-Specific Affine Map Function\n",
    "# ==============================\n",
    "\n",
    "def compute_region_affine(x, w1, b1, w2, b2, w3, b3):\n",
    "    \"\"\"\n",
    "    Forward pass with ReLU masks, collapse to region-specific affine map,\n",
    "    and return region_id encoding activation pattern.\n",
    "    Returns: (A_r, D_r, logits, pred, c1, c2, region_id)\n",
    "    \"\"\"\n",
    "\n",
    "    # Layer 1: input → hidden1\n",
    "    Z1 = w1.dot(x) + b1              # (8,)\n",
    "    S1 = (Z1 > 0).astype(float)      # ReLU mask (8,)\n",
    "    H1 = np.maximum(0, Z1)           # (8,)\n",
    "\n",
    "    # Layer 2: hidden1 → hidden2\n",
    "    Z2 = w2.dot(H1) + b2             # (4,)\n",
    "    S2 = (Z2 > 0).astype(float)      # ReLU mask (4,)\n",
    "    H2 = np.maximum(0, Z2)           # (4,)\n",
    "\n",
    "    # Layer 3: hidden2 → output logits\n",
    "    logits = w3.dot(H2) + b3         # (3,)\n",
    "    pred   = int(np.argmax(logits))  # scalar\n",
    "\n",
    "    # Collapsed affine map (region-specific)\n",
    "    W2m = w2 * S1[None, :]           # (4, 8)\n",
    "    W3m = w3 * S2[None, :]           # (3, 4)\n",
    "    A_r = W3m.dot(W2m.dot(w1))       # (3, 4)\n",
    "    D_r = b3 + W3m.dot(b2) + W3m.dot(W2m.dot(b1))  # (3,)\n",
    "\n",
    "    # Neuron-level contributions to predicted class\n",
    "    c2 = H2 * W3m[pred]              # (4,)\n",
    "    chain = W3m[pred].dot(W2m)       # (8,)\n",
    "    c1 = H1 * chain                  # (8,)\n",
    "\n",
    "    # Region ID: stable, layer-separated bitstring (e.g., \"L1:10101010|L2:0110\")\n",
    "    S1_bits = ''.join(str(int(b)) for b in S1)     # length 8, fixed order\n",
    "    S2_bits = ''.join(str(int(b)) for b in S2)     # length 4, fixed order\n",
    "    region_id = f\"L1:{S1_bits}|L2:{S2_bits}\"\n",
    "\n",
    "    return A_r, D_r, logits, pred, c1, c2, region_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64211a4-a9f3-4509-b244-9e6d21fda46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Feature Contribution Explanation Function\n",
    "# ==============================\n",
    "def explain_region(x, A_r, D_r, logits, pred, feature_names, class_names):\n",
    "    \"\"\"\n",
    "    Given affine map (A_r, D_r) for a sample, compute:\n",
    "      - Logit-level feature contributions\n",
    "      - Probability-level contributions via softmax Jacobian\n",
    "    Returns a DataFrame with contributions per feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Logit-level contributions for predicted class\n",
    "    A_c = A_r[pred]                  # Row of A_r for predicted class\n",
    "    f_logit = A_c * x                # Feature × coefficient\n",
    "\n",
    "    # Compute class probabilities\n",
    "    logits_r = A_r.dot(x) + D_r\n",
    "    probs = softmax(logits_r)\n",
    "\n",
    "    # Softmax Jacobian for predicted class\n",
    "    J = -probs[pred] * probs\n",
    "    J[pred] = probs[pred] * (1 - probs[pred])\n",
    "\n",
    "    # Probability-level contributions\n",
    "    f_all = A_r * x\n",
    "    f_prob = J.dot(f_all)\n",
    "\n",
    "    # Build DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'LogitContribution': f_logit,\n",
    "        'ProbContribution': f_prob,\n",
    "        'PredProbability': probs[pred]\n",
    "    }, index=feature_names).round(4)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f91f6-bedd-493b-a274-71b52b4b5c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Use final parameters\n",
    "# ==============================\n",
    "w1, b1 = w1_final, b1_final     # (8, 4), (8,)\n",
    "w2, b2 = w2_final, b2_final     # (4, 8), (4,)\n",
    "w3, b3 = w3_final, b3_final     # (3, 4), (3,)\n",
    "\n",
    "# ==============================\n",
    "# Prepare data & names\n",
    "# ==============================\n",
    "feature_names = ['Sepal length', 'Sepal width', 'Petal length', 'Petal width']\n",
    "class_names   = le.classes_.tolist()\n",
    "\n",
    "# ==============================\n",
    "# TRAIN set: explanations, neuron contributions, and regions\n",
    "# ==============================\n",
    "X_train_np = X_train.numpy()                 # (N_train, 4)\n",
    "N_train, _ = X_train_np.shape\n",
    "\n",
    "all_explanations_train = []                  # per-sample feature contributions\n",
    "neuron1_train = []                           # L1 neuron contributions per sample\n",
    "neuron2_train = []                           # L2 neuron contributions per sample\n",
    "rank_counts_train = {f: [0]*len(feature_names) for f in feature_names}\n",
    "y_train_preds = np.zeros(N_train, dtype=int) # predictions\n",
    "train_regions = {}                           # region_id -> list of train indices\n",
    "\n",
    "for i in range(N_train):\n",
    "    x = X_train_np[i]\n",
    "\n",
    "    # Collapse to region-specific affine and get region id\n",
    "    A_r, D_r, logits, pred, c1, c2, region_id = compute_region_affine(x, w1, b1, w2, b2, w3, b3)\n",
    "    y_train_preds[i] = pred\n",
    "\n",
    "    # Feature-level explanation\n",
    "    df_exp = explain_region(x, A_r, D_r, logits, pred, feature_names, class_names)\n",
    "    df_exp.insert(0, 'Sample', i)\n",
    "    all_explanations_train.append(df_exp)\n",
    "\n",
    "    # Rank features by absolute probability contribution\n",
    "    ranking = np.argsort(-df_exp['ProbContribution'].abs().values)\n",
    "    for rank, feat_idx in enumerate(ranking):\n",
    "        rank_counts_train[feature_names[feat_idx]][rank] += 1\n",
    "\n",
    "    # Store neuron contributions\n",
    "    neuron1_train.append(c1)    # shape (8,)\n",
    "    neuron2_train.append(c2)    # shape (4,)\n",
    "\n",
    "    # Accumulate region membership\n",
    "    train_regions.setdefault(region_id, []).append(i)\n",
    "\n",
    "# Tidy outputs (optional tables)\n",
    "big_df_train = (\n",
    "    pd.concat(all_explanations_train)\n",
    "      .reset_index().rename(columns={'index': 'Feature'})\n",
    "      .set_index(['Sample','Feature'])\n",
    ")\n",
    "neuron1_train = np.stack(neuron1_train)      # (N_train, 8)\n",
    "neuron2_train = np.stack(neuron2_train)      # (N_train, 4)\n",
    "rank_df_train = pd.DataFrame(\n",
    "    rank_counts_train,\n",
    "    index=[f'Rank {r+1}' for r in range(len(feature_names))]\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# TEST set: explanations, neuron contributions, and regions\n",
    "# ==============================\n",
    "X_test_np = X_test.numpy()                   # (N_test, 4)\n",
    "N_test, _ = X_test_np.shape\n",
    "\n",
    "all_explanations_test = []\n",
    "neuron1_test = []\n",
    "neuron2_test = []\n",
    "rank_counts_test = {f: [0]*len(feature_names) for f in feature_names}\n",
    "y_test_preds = np.zeros(N_test, dtype=int)\n",
    "test_regions = {}                            # region_id -> list of test indices\n",
    "\n",
    "for i in range(N_test):\n",
    "    x = X_test_np[i]\n",
    "\n",
    "    # Collapse to region-specific affine and get region id\n",
    "    A_r, D_r, logits, pred, c1, c2, region_id = compute_region_affine(x, w1, b1, w2, b2, w3, b3)\n",
    "    y_test_preds[i] = pred\n",
    "\n",
    "    # Feature-level explanation\n",
    "    df_exp = explain_region(x, A_r, D_r, logits, pred, feature_names, class_names)\n",
    "    df_exp.insert(0, 'Sample', i)\n",
    "    all_explanations_test.append(df_exp)\n",
    "\n",
    "    # Rank features by absolute probability contribution\n",
    "    ranking = np.argsort(-df_exp['ProbContribution'].abs().values)\n",
    "    for rank, feat_idx in enumerate(ranking):\n",
    "        rank_counts_test[feature_names[feat_idx]][rank] += 1\n",
    "\n",
    "    # Store neuron contributions\n",
    "    neuron1_test.append(c1)                 # (4,) → c2 is for layer 2; c1 is for layer 1 (8,)\n",
    "    neuron2_test.append(c2)                 # (4,)\n",
    "\n",
    "    # Accumulate region membership\n",
    "    test_regions.setdefault(region_id, []).append(i)\n",
    "\n",
    "# Tidy outputs (optional tables)\n",
    "big_df_test = (\n",
    "    pd.concat(all_explanations_test)\n",
    "      .reset_index().rename(columns={'index': 'Feature'})\n",
    "      .set_index(['Sample','Feature'])\n",
    ")\n",
    "neuron1_test = np.stack(neuron1_test)        # (N_test, 8)\n",
    "neuron2_test = np.stack(neuron2_test)        # (N_test, 4)\n",
    "rank_df_test = pd.DataFrame(\n",
    "    rank_counts_test,\n",
    "    index=[f'Rank {r+1}' for r in range(len(feature_names))]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65779fc2-a107-40fb-b09e-993bcaa75938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Region analysis: overlap, purity, coverage, feature profiles (TEST set)\n",
    "# ==============================\n",
    "\n",
    "\n",
    "def region_analysis(\n",
    "    train_regions,             # dict: region_id -> list(train idx)\n",
    "    test_regions,              # dict: region_id -> list(test idx)\n",
    "    X_test_np,                 # (N_test, d)\n",
    "    y_test_np,                 # (N_test,)\n",
    "    y_test_preds,              # (N_test,)\n",
    "    feature_names,             # list[str], length d\n",
    "    class_names,               # list[str], length C\n",
    "    w1, b1, w2, b2, w3, b3,    # final parameters\n",
    "    purity_threshold=0.7\n",
    "):\n",
    "    # Overlap counts\n",
    "    train_ids = set(train_regions.keys())\n",
    "    test_ids  = set(test_regions.keys())\n",
    "    n_regions_train = len(train_ids)\n",
    "    n_regions_test  = len(test_ids)\n",
    "    overlap_count   = len(train_ids & test_ids)\n",
    "    test_only_count = len(test_ids - train_ids)\n",
    "    overlap_pct     = overlap_count / max(1, n_regions_test)\n",
    "\n",
    "    # Per-region class purity (TEST) uses ground truth\n",
    "    C = len(class_names)\n",
    "    rows = []\n",
    "    for region_id, idxs in test_regions.items():\n",
    "        if not idxs:\n",
    "            continue\n",
    "        labels = y_test_np[idxs]                      # ground-truth labels\n",
    "        counts = np.bincount(labels, minlength=C)     # class histogram\n",
    "        n_r = len(idxs)\n",
    "        purity = (counts.max() / n_r) if n_r > 0 else 0.0\n",
    "        is_confusion = (purity < purity_threshold)\n",
    "\n",
    "        row = {'region_id': region_id, 'n_r': n_r, 'purity': purity, 'is_confusion': is_confusion}\n",
    "        for c in range(C):\n",
    "            row[f'count_{class_names[c]}'] = int(counts[c])\n",
    "        rows.append(row)\n",
    "\n",
    "    region_summary_test = (\n",
    "        pd.DataFrame(rows)\n",
    "          .set_index('region_id')\n",
    "          .sort_values(by='n_r', ascending=False)\n",
    "    )\n",
    "\n",
    "    # Coverage metrics (TEST)\n",
    "    N_test = len(y_test_np)\n",
    "    region_sizes = sorted((len(v) for v in test_regions.values()), reverse=True)\n",
    "    top1_coverage = (region_sizes[0] / N_test) if region_sizes else 0.0\n",
    "    topC_coverage = (sum(region_sizes[:C]) / N_test) if region_sizes else 0.0\n",
    "\n",
    "    # Region-wise feature profiles (TEST)\n",
    "    majority_rows = []\n",
    "    per_class_rows = []\n",
    "    mixture_rows = []\n",
    "\n",
    "    for region_id, idxs in test_regions.items():\n",
    "        if not idxs:\n",
    "            continue\n",
    "\n",
    "        # Region affine map (any sample from region; masks define A_r, D_r)\n",
    "        x0 = X_test_np[idxs[0]]\n",
    "        A_r, D_r, _, _, _, _, _ = compute_region_affine(x0, w1, b1, w2, b2, w3, b3)\n",
    "\n",
    "        # ---------- FIX: majority class by GROUND TRUTH ----------\n",
    "        labels_region = y_test_np[idxs]                          # ground-truth labels\n",
    "        cls_idx = int(np.bincount(labels_region, minlength=C).argmax())\n",
    "        cls_name = class_names[cls_idx]\n",
    "\n",
    "        # Majority profile (use A_r[cls_idx], average over ALL samples in region)\n",
    "        X_reg = X_test_np[idxs]                                  # (n_r, d)\n",
    "        majority_contrib = (A_r[cls_idx][None, :] * X_reg).mean(axis=0)\n",
    "        maj_row = {'region_id': region_id, 'class_used': cls_name}\n",
    "        for j, feat in enumerate(feature_names):\n",
    "            maj_row[feat] = float(np.round(majority_contrib[j], 4))\n",
    "        majority_rows.append(maj_row)\n",
    "\n",
    "        # ---------- Change: per-class profiles by GROUND TRUTH for consistency ----------\n",
    "        for c in range(C):\n",
    "            idxs_c = [k for k in idxs if y_test_np[k] == c]\n",
    "            if not idxs_c:\n",
    "                continue\n",
    "            X_reg_c = X_test_np[idxs_c]                           # (n_rc, d)\n",
    "            pc_contrib = (A_r[c][None, :] * X_reg_c).mean(axis=0)\n",
    "            pc_row = {'region_id': region_id, 'class_used': class_names[c]}\n",
    "            for j, feat in enumerate(feature_names):\n",
    "                pc_row[feat] = float(np.round(pc_contrib[j], 4))\n",
    "            per_class_rows.append(pc_row)\n",
    "\n",
    "        # Mixture profile (keep per PREDICTED class behavior)\n",
    "        mix_contribs = []\n",
    "        for k in idxs:\n",
    "            c_pred = int(y_test_preds[k])\n",
    "            mix_contribs.append(A_r[c_pred] * X_test_np[k])\n",
    "        mix_profile = np.mean(np.stack(mix_contribs, axis=0), axis=0)\n",
    "        mix_row = {'region_id': region_id, 'class_used': 'mixture'}\n",
    "        for j, feat in enumerate(feature_names):\n",
    "            mix_row[feat] = float(np.round(mix_profile[j], 4))\n",
    "        mixture_rows.append(mix_row)\n",
    "\n",
    "    # Build DataFrames\n",
    "    region_feature_profile_test = (\n",
    "        pd.DataFrame(majority_rows)\n",
    "          .set_index('region_id')\n",
    "          .sort_index()\n",
    "    )\n",
    "    region_feature_profile_test_per_class = (\n",
    "        pd.DataFrame(per_class_rows)\n",
    "          .set_index(['region_id', 'class_used'])\n",
    "          .sort_index()\n",
    "    )\n",
    "    region_feature_profile_test_mixture = (\n",
    "        pd.DataFrame(mixture_rows)\n",
    "          .set_index('region_id')\n",
    "          .sort_index()\n",
    "    )\n",
    "\n",
    "    # Outputs\n",
    "    return {\n",
    "        'n_regions_train': n_regions_train,\n",
    "        'n_regions_test' : n_regions_test,\n",
    "        'overlap_count'  : overlap_count,\n",
    "        'test_only_count': test_only_count,\n",
    "        'overlap_pct'    : overlap_pct,\n",
    "        'region_summary_test'                : region_summary_test,\n",
    "        'top1_coverage'                      : top1_coverage,\n",
    "        'topC_coverage'                      : topC_coverage,\n",
    "        'region_feature_profile_test'        : region_feature_profile_test,              # majority (by GT)\n",
    "        'region_feature_profile_test_per_class': region_feature_profile_test_per_class,  # per-class (by GT)\n",
    "        'region_feature_profile_test_mixture': region_feature_profile_test_mixture       # mixture (by pred)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04502105-dde5-4ac1-b4ea-a58ada12169e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Example usage\n",
    "# ==============================\n",
    "\n",
    "# Final parameters\n",
    "w1, b1 = w1_final, b1_final\n",
    "w2, b2 = w2_final, b2_final\n",
    "w3, b3 = w3_final, b3_final\n",
    "\n",
    "# Arrays\n",
    "X_test_np = X_test.numpy()\n",
    "y_test_np = y_test.numpy()\n",
    "\n",
    "# Run analysis\n",
    "results = region_analysis(\n",
    "    train_regions=train_regions,\n",
    "    test_regions=test_regions,\n",
    "    X_test_np=X_test_np,\n",
    "    y_test_np=y_test_np,\n",
    "    y_test_preds=y_test_preds,\n",
    "    feature_names=feature_names,\n",
    "    class_names=class_names,\n",
    "    w1=w1, b1=b1, w2=w2, b2=b2, w3=w3, b3=b3,\n",
    "    purity_threshold=0.7\n",
    ")\n",
    "\n",
    "# Quick prints\n",
    "print(f\"Train regions: {results['n_regions_train']}\")\n",
    "print(f\"Test regions: {results['n_regions_test']}\")\n",
    "print(f\"Overlap (count): {results['overlap_count']}\")\n",
    "print(f\"Test-only regions: {results['test_only_count']}\")\n",
    "print(f\"Overlap (% of test regions): {results['overlap_pct']:.2%}\")\n",
    "print(f\"Top-1 coverage (test): {results['top1_coverage']:.2%}\")\n",
    "print(f\"Top-{len(class_names)} coverage (test): {results['topC_coverage']:.2%}\")\n",
    "\n",
    "print(\"\\n=== Region summary (test) ===\")\n",
    "print(results['region_summary_test'].head(10))\n",
    "\n",
    "print(\"\\n=== Region-wise feature profile (majority class, test) ===\")\n",
    "print(results['region_feature_profile_test'].head(10))\n",
    "\n",
    "print(\"\\n=== Region-wise feature profile (per-class, test) ===\")\n",
    "print(results['region_feature_profile_test_per_class'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097dc500-096d-46b4-a01d-74dcb807e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Super-Neuron Identification — Two Hidden Layers\n",
    "# ==========================================\n",
    "\n",
    "neuron1_matrix = neuron1_train\n",
    "neuron2_matrix = neuron2_train\n",
    "\n",
    "for top_k in [1, 2, 3]:\n",
    "    freq1 = np.zeros(neuron1_matrix.shape[1], dtype=int)\n",
    "    freq2 = np.zeros(neuron2_matrix.shape[1], dtype=int)\n",
    "\n",
    "    for c1, c2 in zip(neuron1_matrix, neuron2_matrix):\n",
    "        # Rank neurons by absolute contribution magnitude\n",
    "        top1 = np.argsort(-np.abs(c1))[:top_k]\n",
    "        top2 = np.argsort(-np.abs(c2))[:top_k]\n",
    "        freq1[top1] += 1\n",
    "        freq2[top2] += 1\n",
    "\n",
    "    super1 = int(np.argmax(freq1))\n",
    "    super2 = int(np.argmax(freq2))\n",
    "\n",
    "    print(f\"Top-{top_k} super-neuron (Layer 1): #{super1} \"\n",
    "          f\"(appears in top-{top_k} for {freq1[super1]} samples)\")\n",
    "    print(f\"Top-{top_k} super-neuron (Layer 2): #{super2} \"\n",
    "          f\"(appears in top-{top_k} for {freq2[super2]} samples)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# check neuron activity\n",
    "active1 = np.sum(np.abs(neuron1_train) > 1e-6, axis=0) > 0\n",
    "active2 = np.sum(np.abs(neuron2_train) > 1e-6, axis=0) > 0\n",
    "\n",
    "print(f\"Layer 1: {active1.sum()}/{len(active1)} neurons active, \"\n",
    "      f\"{(~active1).sum()} inactive\")\n",
    "print(f\"Layer 2: {active2.sum()}/{len(active2)} neurons active, \"\n",
    "      f\"{(~active2).sum()} inactive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a0da7-86a0-44d1-abaf-c602422cdd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Super-Neuron Identification — Two Hidden Layers\n",
    "# ==========================================\n",
    "\n",
    "neuron1_matrix = neuron1_test\n",
    "neuron2_matrix = neuron2_test\n",
    "\n",
    "for top_k in [1, 2, 3]:\n",
    "    freq1 = np.zeros(neuron1_matrix.shape[1], dtype=int)\n",
    "    freq2 = np.zeros(neuron2_matrix.shape[1], dtype=int)\n",
    "\n",
    "    for c1, c2 in zip(neuron1_matrix, neuron2_matrix):\n",
    "        # Rank neurons by absolute contribution magnitude\n",
    "        top1 = np.argsort(-np.abs(c1))[:top_k]\n",
    "        top2 = np.argsort(-np.abs(c2))[:top_k]\n",
    "        freq1[top1] += 1\n",
    "        freq2[top2] += 1\n",
    "\n",
    "    super1 = int(np.argmax(freq1))\n",
    "    super2 = int(np.argmax(freq2))\n",
    "\n",
    "    print(f\"Top-{top_k} super-neuron (Layer 1): #{super1} \"\n",
    "          f\"(appears in top-{top_k} for {freq1[super1]} samples)\")\n",
    "    print(f\"Top-{top_k} super-neuron (Layer 2): #{super2} \"\n",
    "          f\"(appears in top-{top_k} for {freq2[super2]} samples)\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# check neuron activity\n",
    "active1 = np.sum(np.abs(neuron1_train) > 1e-6, axis=0) > 0\n",
    "active2 = np.sum(np.abs(neuron2_train) > 1e-6, axis=0) > 0\n",
    "\n",
    "print(f\"Layer 1: {active1.sum()}/{len(active1)} neurons active, \"\n",
    "      f\"{(~active1).sum()} inactive\")\n",
    "print(f\"Layer 2: {active2.sum()}/{len(active2)} neurons active, \"\n",
    "      f\"{(~active2).sum()} inactive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7e538b-336b-420b-90e8-c8c8e7391929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
